{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어드레스 자세 탐지 시 머리 초기값 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2.16.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 상태 저장을 위한 전역 변수 선언\n",
    "initial_horizontal_change = None\n",
    "initial_vertical_change = None\n",
    "initial_lateral_change = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Model + Draw Keypoints + Draw Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='movenet_lighting_tflite_float16.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0,255,0), -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어깨 중간과 엉덩이 중간에 세로선 그리기 추가\n",
    "def draw_midline(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "\n",
    "    shoulder_mid = (shaped[5][:2] + shaped[6][:2]) / 2\n",
    "    hip_mid = (shaped[11][:2] + shaped[12][:2]) / 2\n",
    "\n",
    "    if (shaped[5][2] > confidence_threshold and shaped[6][2] > confidence_threshold and\n",
    "            shaped[11][2] > confidence_threshold and shaped[12][2] > confidence_threshold):\n",
    "        cv2.line(frame, (int(shoulder_mid[1]), int(shoulder_mid[0])), (int(hip_mid[1]), int(hip_mid[0])), (255, 0, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_face_vertical_line(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
    "\n",
    "    # 머리 상단과 목 위치를 사용하여 대략적인 이마와 턱의 위치 추정\n",
    "    head_top = shaped[0][:2]  # 머리 상단(이마로 가정)\n",
    "    neck = shaped[1][:2]  # 목(턱 근처로 가정)\n",
    "\n",
    "    if (shaped[0][2] > confidence_threshold and shaped[1][2] > confidence_threshold):\n",
    "        # 이마에서 턱까지의 선 그리기\n",
    "        cv2.line(frame, (int(head_top[1]), int(head_top[0])), (int(neck[1]), int(neck[0])), (0, 0, 255), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_address_pose(keypoints_with_scores, confidence_threshold=0.4):\n",
    "    keypoints = np.squeeze(keypoints_with_scores)\n",
    "    \n",
    "    # 키포인트 신뢰도 체크\n",
    "    if (keypoints[5][2] < confidence_threshold or keypoints[6][2] < confidence_threshold or\n",
    "        keypoints[11][2] < confidence_threshold or keypoints[12][2] < confidence_threshold):\n",
    "        return False  # 신뢰도가 임계값 미만인 키포인트가 있으면 어드레스 자세로 판단하지 않음\n",
    "\n",
    "    # 어깨와 골반의 중간점 계산\n",
    "    shoulder_midpoint = (keypoints[5][:2] + keypoints[6][:2]) / 2\n",
    "    hip_midpoint = (keypoints[11][:2] + keypoints[12][:2]) / 2\n",
    "\n",
    "    # 어깨와 골반의 높이 차이 계산\n",
    "    vertical_diff = abs(shoulder_midpoint[1] - hip_midpoint[1])\n",
    "    \n",
    "    # 머리-목 각도와 목-골반 각도를 계산\n",
    "    head_point = keypoints[0, :2]\n",
    "    neck_point = keypoints[1, :2]\n",
    "    hip_center = (keypoints[11, :2] + keypoints[12, :2]) / 2\n",
    "    head_neck_angle = calculate_angle(head_point, neck_point)\n",
    "    neck_hip_angle = calculate_angle(neck_point, hip_center)\n",
    "    \n",
    "    # 각도 차이를 계산\n",
    "    angle_diff = abs(head_neck_angle - neck_hip_angle)\n",
    "    \n",
    "    # 높이 차이와 각도 차이를 기반으로 어드레스 자세와 몸통의 직선성 판별\n",
    "    vertical_diff_threshold = 5  # 높이 차이 임계값\n",
    "    if vertical_diff < vertical_diff_threshold and (angle_diff < 165 and angle_diff > 145):\n",
    "        return True  # 어드레스 자세이며 몸통이 직선임\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(p1, p2):\n",
    "    \"\"\"두 점 p1, p2 간의 각도를 계산합니다.\"\"\"\n",
    "    angle = np.arctan2(p2[1] - p1[1], p2[0] - p1[0]) * 180.0 / np.pi\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(p1, p2):\n",
    "    \"\"\"두 점 p1, p2 사이의 유클리드 거리를 계산합니다.\"\"\"\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_face_pose(keypoints):\n",
    "#     nose = keypoints[0]  # 코\n",
    "#     left_ear = keypoints[3]  # 왼쪽 귀\n",
    "#     right_ear = keypoints[4]  # 오른쪽 귀\n",
    "#     neck = keypoints[1]  # 목\n",
    "\n",
    "#     # 수평 변화 감지: 귀와 귀 사이의 거리\n",
    "#     horizontal_change = calculate_distance(left_ear[:2], right_ear[:2])\n",
    "#     print(f\"수평 변화 거리: {horizontal_change:.2f}\")\n",
    "\n",
    "#     # 높이 변화 감지: 코와 목 사이의 거리\n",
    "#     vertical_change = calculate_distance(nose[:2], neck[:2])\n",
    "#     print(f\"높이 변화 거리: {vertical_change:.2f}\")\n",
    "\n",
    "#     # 좌우 거리 변화 감지: 코와 양쪽 귀의 중점 사이의 거리\n",
    "#     ears_midpoint = ((left_ear[0] + right_ear[0]) / 2, (left_ear[1] + right_ear[1]) / 2)\n",
    "#     lateral_change = calculate_distance(nose[:2], ears_midpoint)\n",
    "#     print(f\"좌우 거리 변화: {lateral_change:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    \"\"\"두 점 p1, p2 사이의 거리를 계산합니다.\"\"\"\n",
    "    return np.sqrt((p2[0] - p1[0]) ** 2 + (p2[1] - p1[1]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_initial_state_face(keypoints):\n",
    "    global initial_horizontal_change, initial_vertical_change, initial_lateral_change\n",
    "    # 초기 상태 계산 및 저장\n",
    "    nose = keypoints[0]\n",
    "    left_ear = keypoints[3]\n",
    "    right_ear = keypoints[4]\n",
    "    neck = keypoints[1]\n",
    "\n",
    "    initial_horizontal_change = calculate_distance(left_ear[:2], right_ear[:2])\n",
    "    initial_vertical_change = calculate_distance(nose[:2], neck[:2])\n",
    "    ears_midpoint = ((left_ear[0] + right_ear[0]) / 2, (left_ear[1] + right_ear[1]) / 2)\n",
    "    initial_lateral_change = calculate_distance(nose[:2], ears_midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_swing_from_initial_face(keypoints):\n",
    "    # 현재 상태 계산\n",
    "    nose = keypoints[0]\n",
    "    left_ear = keypoints[3]\n",
    "    right_ear = keypoints[4]\n",
    "    neck = keypoints[1]\n",
    "\n",
    "    current_horizontal_change = calculate_distance(left_ear[:2], right_ear[:2])\n",
    "    current_vertical_change = calculate_distance(nose[:2], neck[:2])\n",
    "    ears_midpoint = ((left_ear[0] + right_ear[0]) / 2, (left_ear[1] + right_ear[1]) / 2)\n",
    "    current_lateral_change = calculate_distance(nose[:2], ears_midpoint)\n",
    "\n",
    "    # 초기 상태와 비교\n",
    "    horizontal_movement = current_horizontal_change - initial_horizontal_change\n",
    "    vertical_movement = current_vertical_change - initial_vertical_change\n",
    "    lateral_movement = current_lateral_change - initial_lateral_change\n",
    "\n",
    "    print(f\"스윙 수평 변화: {horizontal_movement:.2f}, 스윙 높이 변화: {vertical_movement:.2f}, 스윙 좌우 변화: {lateral_movement:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 316,\n",
       "  'shape': array([ 1,  1, 17,  3]),\n",
       "  'shape_signature': array([ 1,  1, 17,  3]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaped = np.squeeze(np.multiply(interpreter.get_tensor(interpreter.get_output_details()[0]['index']), [480,640,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "for edge, color in EDGES.items():\n",
    "    p1, p2 = edge\n",
    "    y1, x1, c1 = shaped[p1]\n",
    "    y2, x2, c2 = shaped[p2]\n",
    "    print((int(x2), int(y2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n",
      "0 0 0.0\n"
     ]
    }
   ],
   "source": [
    "for kp in shaped:\n",
    "    ky, kx, kp_conf = kp\n",
    "    print(int(ky), int(kx), kp_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올바른 어드레스 자세 감지됨. 초기 상태를 저장합니다.\n",
      "스윙 수평 변화: 1.54, 스윙 높이 변화: -6.92, 스윙 좌우 변화: -0.77\n",
      "스윙 수평 변화: 2.77, 스윙 높이 변화: -7.45, 스윙 좌우 변화: 0.24\n",
      "스윙 수평 변화: 2.24, 스윙 높이 변화: -6.55, 스윙 좌우 변화: -0.58\n",
      "스윙 수평 변화: 4.60, 스윙 높이 변화: -2.82, 스윙 좌우 변화: 9.62\n",
      "스윙 수평 변화: 7.27, 스윙 높이 변화: 0.04, 스윙 좌우 변화: 6.06\n",
      "스윙 수평 변화: 4.28, 스윙 높이 변화: -3.14, 스윙 좌우 변화: 7.41\n",
      "스윙 수평 변화: 3.89, 스윙 높이 변화: -4.04, 스윙 좌우 변화: 5.99\n",
      "스윙 수평 변화: 5.26, 스윙 높이 변화: -2.89, 스윙 좌우 변화: 7.95\n",
      "스윙 수평 변화: 3.59, 스윙 높이 변화: -5.83, 스윙 좌우 변화: 4.41\n",
      "스윙 수평 변화: 5.44, 스윙 높이 변화: -4.85, 스윙 좌우 변화: 5.07\n",
      "스윙 수평 변화: 13.08, 스윙 높이 변화: 1.19, 스윙 좌우 변화: 4.52\n",
      "스윙 수평 변화: 12.21, 스윙 높이 변화: -1.48, 스윙 좌우 변화: 4.15\n",
      "스윙 수평 변화: 11.68, 스윙 높이 변화: -0.20, 스윙 좌우 변화: 2.62\n",
      "스윙 수평 변화: 11.92, 스윙 높이 변화: 0.09, 스윙 좌우 변화: 5.80\n",
      "스윙 수평 변화: 13.12, 스윙 높이 변화: 0.72, 스윙 좌우 변화: 5.13\n",
      "스윙 수평 변화: 13.54, 스윙 높이 변화: 2.24, 스윙 좌우 변화: 6.80\n",
      "스윙 수평 변화: 13.09, 스윙 높이 변화: 1.73, 스윙 좌우 변화: 5.72\n",
      "스윙 수평 변화: 15.36, 스윙 높이 변화: 1.38, 스윙 좌우 변화: 3.27\n",
      "스윙 수평 변화: 20.38, 스윙 높이 변화: -0.29, 스윙 좌우 변화: 12.28\n",
      "스윙 수평 변화: 22.22, 스윙 높이 변화: 1.46, 스윙 좌우 변화: 14.25\n",
      "스윙 수평 변화: 28.98, 스윙 높이 변화: -1.29, 스윙 좌우 변화: 19.18\n",
      "스윙 수평 변화: 28.98, 스윙 높이 변화: -1.29, 스윙 좌우 변화: 19.18\n",
      "스윙 수평 변화: 38.75, 스윙 높이 변화: 3.03, 스윙 좌우 변화: 19.85\n",
      "스윙 수평 변화: 71.41, 스윙 높이 변화: 13.89, 스윙 좌우 변화: 46.70\n",
      "스윙 수평 변화: 78.82, 스윙 높이 변화: 15.78, 스윙 좌우 변화: 46.67\n",
      "스윙 수평 변화: 89.85, 스윙 높이 변화: 19.44, 스윙 좌우 변화: 45.54\n",
      "스윙 수평 변화: 101.36, 스윙 높이 변화: 22.55, 스윙 좌우 변화: 54.08\n",
      "스윙 수평 변화: 108.30, 스윙 높이 변화: 23.09, 스윙 좌우 변화: 61.76\n",
      "스윙 수평 변화: 108.30, 스윙 높이 변화: 23.09, 스윙 좌우 변화: 61.76\n",
      "스윙 수평 변화: 137.55, 스윙 높이 변화: 34.20, 스윙 좌우 변화: 68.35\n",
      "스윙 수평 변화: 138.89, 스윙 높이 변화: 36.91, 스윙 좌우 변화: 70.24\n",
      "스윙 수평 변화: 141.20, 스윙 높이 변화: 32.14, 스윙 좌우 변화: 77.32\n",
      "스윙 수평 변화: 132.72, 스윙 높이 변화: 32.17, 스윙 좌우 변화: 64.22\n",
      "스윙 수평 변화: 130.77, 스윙 높이 변화: 34.95, 스윙 좌우 변화: 45.68\n",
      "스윙 수평 변화: 125.57, 스윙 높이 변화: 26.72, 스윙 좌우 변화: 37.12\n",
      "스윙 수평 변화: 121.00, 스윙 높이 변화: 23.72, 스윙 좌우 변화: 36.27\n",
      "스윙 수평 변화: 102.32, 스윙 높이 변화: 17.35, 스윙 좌우 변화: 36.35\n",
      "스윙 수평 변화: 15.06, 스윙 높이 변화: 6.11, 스윙 좌우 변화: 25.18\n",
      "스윙 수평 변화: -29.66, 스윙 높이 변화: -13.68, 스윙 좌우 변화: 12.07\n",
      "스윙 수평 변화: 30.49, 스윙 높이 변화: 8.08, 스윙 좌우 변화: 10.78\n",
      "스윙 수평 변화: 101.10, 스윙 높이 변화: 8.35, 스윙 좌우 변화: 23.03\n",
      "스윙 수평 변화: 60.81, 스윙 높이 변화: 1.54, 스윙 좌우 변화: 19.53\n",
      "스윙 수평 변화: 99.91, 스윙 높이 변화: 14.70, 스윙 좌우 변화: 37.51\n",
      "스윙 수평 변화: 79.03, 스윙 높이 변화: 9.98, 스윙 좌우 변화: 25.76\n",
      "스윙 수평 변화: 92.08, 스윙 높이 변화: 23.99, 스윙 좌우 변화: 43.49\n",
      "스윙 수평 변화: 100.29, 스윙 높이 변화: 21.26, 스윙 좌우 변화: 42.26\n",
      "스윙 수평 변화: 122.88, 스윙 높이 변화: 33.01, 스윙 좌우 변화: 40.13\n",
      "스윙 수평 변화: 112.12, 스윙 높이 변화: 26.43, 스윙 좌우 변화: 34.90\n",
      "스윙 수평 변화: 105.24, 스윙 높이 변화: 25.73, 스윙 좌우 변화: 28.14\n",
      "스윙 수평 변화: 101.08, 스윙 높이 변화: 22.30, 스윙 좌우 변화: 26.35\n",
      "스윙 수평 변화: -29.47, 스윙 높이 변화: -0.09, 스윙 좌우 변화: 20.68\n",
      "스윙 수평 변화: 103.34, 스윙 높이 변화: 27.89, 스윙 좌우 변화: 40.22\n",
      "스윙 수평 변화: 103.92, 스윙 높이 변화: 22.42, 스윙 좌우 변화: 30.21\n",
      "스윙 수평 변화: 105.81, 스윙 높이 변화: 22.91, 스윙 좌우 변화: 33.55\n",
      "스윙 수평 변화: 108.33, 스윙 높이 변화: 18.11, 스윙 좌우 변화: 37.03\n",
      "스윙 수평 변화: 110.17, 스윙 높이 변화: 22.81, 스윙 좌우 변화: 36.39\n",
      "스윙 수평 변화: 113.20, 스윙 높이 변화: 23.47, 스윙 좌우 변화: 39.15\n",
      "스윙 수평 변화: 120.14, 스윙 높이 변화: 22.68, 스윙 좌우 변화: 37.69\n",
      "스윙 수평 변화: 121.84, 스윙 높이 변화: 22.07, 스윙 좌우 변화: 37.60\n",
      "스윙 수평 변화: 120.07, 스윙 높이 변화: 26.31, 스윙 좌우 변화: 40.13\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 어드레스 자세가 감지되었는지 여부를 추적하는 플래그\n",
    "address_pose_detected = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Reshape image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "    \n",
    "    # input_image = tf.cast(img, dtype=tf.float32)\n",
    "    # 모델이 요구하는 입력 타입에 맞게 타입 변환\n",
    "    input_image = tf.cast(img, dtype=tf.uint8)  # dtype을 tf.float32에서 tf.uint8로 변경\n",
    "    \n",
    "    # Setup input and output \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Make predictions \n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    right_eye = keypoints_with_scores[0][0][2]\n",
    "    left_elbow = keypoints_with_scores[0][0][7]\n",
    "\n",
    "    # 키포인트 추출 및 조정\n",
    "    keypoints = np.squeeze(np.multiply(keypoints_with_scores, [frame.shape[0], frame.shape[1], 1]))\n",
    "    keypoints = keypoints[:, :2]  # x, y 좌표만 사용\n",
    "    \n",
    "    # Rendering \n",
    "    draw_connections(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    draw_keypoints(frame, keypoints_with_scores, 0.4)\n",
    "    \n",
    "    # 기존 렌더링 코드 아래에 세로선 그리기 함수 호출 추가\n",
    "    draw_midline(frame, keypoints_with_scores, 0.4)\n",
    "\n",
    "    # # 얼굴 포즈 분석 함수 호출\n",
    "    # analyze_face_pose(keypoints)\n",
    "\n",
    "    # if is_address_pose(keypoints_with_scores, 0.4):\n",
    "    #     print(\"어드레스 자세 감지됨!\")  # 어드레스 자세와 몸통이 똑바로 서 있는 상태 감지\n",
    "    #     time.sleep(1)\n",
    "    # else:\n",
    "    #     print(\"어드레스 자세 아님.\")\n",
    "    #     time.sleep(1)\n",
    "\n",
    "    # 어드레스 자세 감지\n",
    "    if not address_pose_detected and is_address_pose(keypoints_with_scores, 0.4):\n",
    "        print(\"올바른 어드레스 자세 감지됨. 초기 상태를 저장합니다.\")\n",
    "        save_initial_state_face(keypoints)\n",
    "        address_pose_detected = True\n",
    "    \n",
    "    # 어드레스 자세가 감지된 후 스윙 분석\n",
    "    elif address_pose_detected:\n",
    "        analyze_swing_from_initial_face(keypoints)\n",
    "        # 추가적인 스윙 분석 로직이 필요하면 여기에 구현합니다.\n",
    "\n",
    "    \n",
    "    cv2.imshow('MoveNet Lightning', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
